{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vector_Models_Text_Preprocessing_TF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Termiology\n",
        "\n",
        "* Vocabulary: the set of \"all\" words, reasonable subset of words\n",
        "* Corpus: \"A large collection of writings of a specifc kind or on a specific subject.\" Simply refer to ML dataset sometimes.\n",
        "* N-gram: Simply refers to N consecutive items (eg. words, subwords, characters). Two items called bigrams\n",
        "* Vector: A quantity that has both magnitude and direction; An array of scalars\n",
        "* Bag of words: Vector models and classic machine learning models use bag of words while probabilistic models and deep learning do not. \n",
        "* Stop words: Words we wish to ignore. "
      ],
      "metadata": {
        "id": "Wgke9ctviRcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Words"
      ],
      "metadata": {
        "id": "MTPd0IyMvT9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stop words using nltk\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# english can be replaced with other language choice\n",
        "sw = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h0p-8KbvVdl",
        "outputId": "19547ff2-54ac-4326-cccc-27cb10fc3147"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "Convert words to their root word (like walking, walked to walk) to reduce the voculary size and the dimension of the data.\n",
        "\n",
        "* Stemming is very crude - it just chops off the end of the word and the result is not neccessarily a real word.\n",
        "\n",
        "* Lemmatization is more sophisticated, uses actual rules of language and the true root word will be returned."
      ],
      "metadata": {
        "id": "Hkw5BKOHvHEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "* Based on simple heuristics\n",
        "* Eg: Ends with SSES -> Remove ES; Replacement -> Replac (not a real word)\n",
        "* Multiple stemming algorithms (e.g. Porter Stemmer in NLTK)"
      ],
      "metadata": {
        "id": "sS5bywO93-Dh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T6k3PNyJ5zvY",
        "outputId": "a6785974-621b-43c3-88f4-6a0182d14523"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'walk'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "porter.stem(\"walking\") # returns 'walk'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "* Think of it as a lookup table/table of rules\n",
        "* Stemming: \"Better\" -> \"Better\"\n",
        "* Lemmatization: \"Better\" -> \"Good\"\n",
        "* Eg: Was/Is -> Be"
      ],
      "metadata": {
        "id": "U5V4lkve4ku4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appears in NLTK, spaCy and others"
      ],
      "metadata": {
        "id": "YBYX_f745CAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize(\"mice\") # return 'mouse'\n",
        "\n",
        "lemmatizer.lemmatize(\"going\") # return 'going'\n",
        "lemmatizer.lemmatize(\"going\", pos=wordnet.VERB) # returns 'go'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "SFfrI0uq4gI5",
        "outputId": "56dc24a2-2093-4c9c-b38e-028db88bccb2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NLTK Quirk Regarding POS tages\n",
        "\n",
        "* In order to properly use the lemmatizer, we should first do POS tagging.\n",
        "* NLTK can do POS tagging. \n",
        "* POS tags are not compatible with the WordNet Lemmatizer (solve by map tags from one format to another)"
      ],
      "metadata": {
        "id": "_aKEIlXK54ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and Lemmatization applications"
      ],
      "metadata": {
        "id": "oGxMqNna6grg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Search Engines\n",
        "\n",
        "* Query can not only return the exact matches but also relavent results y converting words to root form\n",
        "* Can do rough typing"
      ],
      "metadata": {
        "id": "1Cfu-4_s6ktN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Online Ads\n",
        "\n",
        "* Ads are based on **Keywords**\n",
        "* Advertisers want to match their ads to user search terms"
      ],
      "metadata": {
        "id": "2cAlaqe265Gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and Lemmatization Demo"
      ],
      "metadata": {
        "id": "bOAasRRM9oU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"walked\"))\n",
        "print(porter.stem(\"walking\"))\n",
        "sentence = \"Lemmatization is more sophisticated than stemming\".split()\n",
        "\n",
        "for token in sentence:\n",
        "  print(porter.stem(token), end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUzoId-j5g8I",
        "outputId": "fe0d15d8-3895-45ff-b8a8-8c40fcaa0193"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walk\n",
            "walk\n",
            "lemmat is more sophist than stem "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"walking\"))\n",
        "print(lemmatizer.lemmatize(\"walking\", pos=wordnet.VERB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx66AftJ92qp",
        "outputId": "031e7465-78b7-4c4a-8892-1c62fe3d0252"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "walking\n",
            "walk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "sentence = \"DT has a devoted following\".split()\n",
        "words_and_tags = nltk.pos_tag(sentence)\n",
        "words_and_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_MfQvyf-EtL",
        "outputId": "0afd5d33-6a6d-45e5-c943-8e0f615f2711"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('DT', 'NNP'),\n",
              " ('has', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('devoted', 'VBN'),\n",
              " ('following', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "for word, tag in words_and_tags:\n",
        "  lemma = lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))\n",
        "  print(lemma, end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtPYznOA_qYk",
        "outputId": "d5342f27-76ee-4e8b-a813-032538fef6e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT have a devote following "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Vectorizer"
      ],
      "metadata": {
        "id": "V_yae5jLO3F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "31B0myZWB4fK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UnCAgLwPZTH",
        "outputId": "f11903f1-fc40-4767-ee3b-169c9f98074a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the data\n",
        "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c3uyy1MPkLU",
        "outputId": "22ae9720-ae9c-4387-c057-24da33425732"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-09 17:24:55--  https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv\n",
            "Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166, 2606:4700:3030::ac43:d5a6, ...\n",
            "Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5085081 (4.8M) [text/csv]\n",
            "Saving to: ‘bbc_text_cls.csv’\n",
            "\n",
            "bbc_text_cls.csv    100%[===================>]   4.85M  4.94MB/s    in 1.0s    \n",
            "\n",
            "2022-05-09 17:24:56 (4.94 MB/s) - ‘bbc_text_cls.csv’ saved [5085081/5085081]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('bbc_text_cls.csv')"
      ],
      "metadata": {
        "id": "WVlPIxWzPq3W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CNff9K_tPvLY",
        "outputId": "8e1da6a8-2989-4d7f-d6eb-d123ab372ad8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text    labels\n",
              "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
              "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
              "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
              "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
              "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6dd0195b-1bd2-4912-bc79-3532a6c64925\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6dd0195b-1bd2-4912-bc79-3532a6c64925')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6dd0195b-1bd2-4912-bc79-3532a6c64925 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6dd0195b-1bd2-4912-bc79-3532a6c64925');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = df['text']\n",
        "labels = df['labels']"
      ],
      "metadata": {
        "id": "VWy5EOCEPw7X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the labels (imbalanced class)\n",
        "labels.hist(figsize=(10,5));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "cND9wmkfP2Xf",
        "outputId": "6547c85c-5ce9-4e01-f3af-fe640721e799"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "inputs_train, inputs_test, Ytrain, Ytest = train_test_split(inputs, labels, random_state=123)"
      ],
      "metadata": {
        "id": "l1yyP8saP71f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "ZbByt77eQNen"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform using the count vector (convert text to numbers)\n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test) # no need to fit the test data"
      ],
      "metadata": {
        "id": "U_8dbODGQWuj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(model.score(Xtrain, Ytrain))\n",
        "print(model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUPsJGMsQk4v",
        "outputId": "fcf690f8-666b-4a56-e8a6-a6605448d66b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9922062350119905\n",
            "0.9712746858168761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with stopwords\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test) # no need to fit the test data\n",
        "# Train the model\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(model.score(Xtrain, Ytrain))\n",
        "print(model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhJRhhYxRQJg",
        "outputId": "d047892e-c4d5-42e0-ee1e-ff421da08023"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9928057553956835\n",
            "0.9766606822262118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "ltW62hayRdbw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LemmaTokenizer:\n",
        "  def __init__(self):\n",
        "    self.wnl = WordNetLemmatizer()\n",
        "  \n",
        "  def __call__(self, doc):\n",
        "    tokens = word_tokenize(doc) # aka string split\n",
        "    words_and_tags = nltk.pos_tag(tokens)\n",
        "    return [self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) \\\n",
        "            for word, tag in words_and_tags]"
      ],
      "metadata": {
        "id": "7ey8dyZ-RkAA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with lemmatization\n",
        "# Convert a collection of text documents to a matrix of token counts\n",
        "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer()) \n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test)\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(model.score(Xtrain, Ytrain))\n",
        "print(model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39WxJbnsSNKU",
        "outputId": "ef13ea8f-5132-489f-9330-8936d78c3e8b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9922062350119905\n",
            "0.9676840215439856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StemTokenizer:\n",
        "  def __init__(self):\n",
        "    self.porter = PorterStemmer()\n",
        "  \n",
        "  def __call__(self, doc):\n",
        "    tokens = word_tokenize(doc)\n",
        "    return [self.porter.stem(t) for t in tokens]"
      ],
      "metadata": {
        "id": "7ugvnLDkTDCu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with Stemming\n",
        "# Convert a collection of text documents to a matrix of token counts\n",
        "vectorizer = CountVectorizer(tokenizer=StemTokenizer()) \n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test)\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(model.score(Xtrain, Ytrain))\n",
        "print(model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0umI5r4TVXB",
        "outputId": "5a2fcc55-e066-4a94-c368-0a287817fd71"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9892086330935251\n",
            "0.9694793536804309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenizer(s):\n",
        "  return s.split()"
      ],
      "metadata": {
        "id": "1fv2-Y1RTgQc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tokenize\n",
        "# string split tokenizer\n",
        "vectorizer = CountVectorizer(tokenizer=simple_tokenizer) \n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test)\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(model.score(Xtrain, Ytrain))\n",
        "print(model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEZESOalTl-X",
        "outputId": "5e834d03-8161-4967-b9ab-d5661b443a6e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9952038369304557\n",
            "0.9712746858168761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Similarity\n",
        "\n",
        "* Can be used in word/article replacement\n",
        "* Measured by Euclidean Distance \n",
        "* Cosine Similarity -> cos(\"angle\")\n",
        "* Cosine Distance = 1 - Cosine Similarity"
      ],
      "metadata": {
        "id": "brYNEqDCUKj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF - Term Frequency - Inverse Document Frequency\n",
        "\n",
        "* Stopwords are unlikely to be useful for NLP tasks.\n",
        "* Stopwords may be application-specific.\n",
        "\n",
        "TF - IDF ~= Term Frequency (Count) /Document Frequency (Words appear in how many documents)\n",
        "\n",
        "ifidf(t,d) = tf(t,d)* idf(t)\n",
        "* t: which term t we are counting.\n",
        "* d: which document d we are looking at.\n",
        "* tf(t,d) = # of times t appears in d.\n",
        "* idf(t) = log(N/N(t)) \n",
        "  * N(t) is the number of documents term t appears in.\n",
        "  * N is the total number of documents\n",
        "\n",
        "\n",
        "\n",
        "### Main Idea Behind TF-IDF\n",
        "\n",
        "* Words that we want to ignore appear in many different documents.\n",
        "* They won't help us differentiate between documents.\n",
        "* We want to somehow scale down these word counts.\n"
      ],
      "metadata": {
        "id": "kaKRJ59CzO9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "Xtrain = tfidf.fit_transform(#train_texts)\n",
        "Xtest = tfidf.transform(#test_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "LkXuwBvCT4sH",
        "outputId": "44fa71d5-fe22-4081-89b9-853537ebcac7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-3235d95c4376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_texts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Term Frequency Variations\n",
        "\n",
        "* Binary (1 if word appears, 0 otherwise).\n",
        "* Normalize the count (sometimes, this is the default)\n",
        "  tf(t,d) = count(t,d)/sum(count(t',d)) for t' belongs to terms(d)\n",
        "* Take the log\n",
        "  tf(t,d) = log(1+count(t,d))\n"
      ],
      "metadata": {
        "id": "PtCRFBwa24DM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inverse Document Frequency Variations\n",
        "\n",
        "* Smooth IDF\n",
        "  idf(t) = log(N/(N(t)+1) + 1\n",
        "* IDF Max\n",
        "  idf(t) = log(max(N(t'))/N(t))\n",
        "* Probabilistic IDF\n",
        "  idf(t) = log((N - N(t))/N(t))"
      ],
      "metadata": {
        "id": "YleZEA7a4exW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-to-Index Mapping\n",
        "\n",
        "* When converting documents to vectors, the result is a document-term matrix.\n",
        "* Row = document, column = term (size = # documents * # terms)"
      ],
      "metadata": {
        "id": "1XoGmUok-rzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sudo codes\n",
        "current_idx = 0\n",
        "word2idx = {}\n",
        "\n",
        "for doc in documents:\n",
        "  tokens = word_tokenize(doc)\n",
        "  for token in tokens:\n",
        "    if token not in word2idx:\n",
        "      word2idx[token] = current_idx\n",
        "      current_idx += 1\n",
        "  "
      ],
      "metadata": {
        "id": "xNTvTKlEyaeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train test split\n",
        "\n",
        "* Method 1: ignore words from the training set\n",
        "* Method 2: create a special index for unknown/rare words. \n",
        "  * Assign any rare words (those frquency falls below a threshold) to the unknown index. "
      ],
      "metadata": {
        "id": "GfMY8-NAKZRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reverse Mapping (Index to word)"
      ],
      "metadata": {
        "id": "TENojk9MLJpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Word Embeddings\n",
        "\n",
        "* Can convert a document into a vector (But not sparse like counting/TFIDF)\n",
        "* Embeddings are dense and low-dimensional\n"
      ],
      "metadata": {
        "id": "9gBDNUJRLPe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2vec (Google)\n",
        "\n",
        "* Embeddings are stored in the weights of the neural network.\n",
        "* Goal of training: given an input word, predict whether an output word appears in its context which the given word shows. "
      ],
      "metadata": {
        "id": "TO32lBJgXoIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Glove (Stanford)\n",
        "\n",
        "* Not use neural networks, but invented in the era of \"Deep NLP\"\n",
        "* Embeddings are still used in neural network at later stages"
      ],
      "metadata": {
        "id": "wpH5fYNDXo22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "\n",
        "* Connections between text (strings) and numbers\n",
        "\n",
        "* Text preprocessing:\n",
        "  * Tokenization\n",
        "  * Bag of words\n",
        "  * Stopwords\n",
        "  * Stemming and lemmatization\n",
        "\n",
        "* Convert text into vectors\n",
        "  * Counting\n",
        "  * TF-IDF\n",
        "  * Vector similarity/recommender system\n",
        "  * Word-to-index mapping\n",
        "  * Neural word embeddings (word2ve, Glove)\n",
        "  * Word analogies"
      ],
      "metadata": {
        "id": "6R04GPa_cOtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps of a Typical NLP Analysis\n",
        "* Get the text (strings)\n",
        "* Tokenize the text\n",
        "* Stopwords, stemming/lemmatization\n",
        "* Map tokens to integers\n",
        "  * Tabular ML works with numbers\n",
        "  * A table of the format (documents * tokens)\n",
        "  * Need to know which column goes with which token\n",
        "* Convert text into count vectors/TF-IDF\n",
        "* ML task "
      ],
      "metadata": {
        "id": "9f8tDfF4dyOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JklqH9v7JovL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}