\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{mathabx}
\usepackage{tikz}

\setlength\parindent{0pt}

\title{CS 4780/5780 Homework 2}
\author{Due: Wednesday 02/21/18 11:55pm on Gradescope}
\date{}

\begin{document}
\maketitle

The following problems concern the perceptron. In class you learned when the perceptron may be applied, how to algorithmically train the perceptron, and when the perceptron converges.

\subsection*{Problem 1}
% Learning goals: perceptron prediction formula, error formula for perceptrons, training error is 0.
Suppose your boss Celeste gives you a fully trained perceptron ($w$), the training set used to train it ($D_{TR}$), and a separate test set ($D_{TE}$). Here $x\in\mathbb{R}^d$ and $y\in\{-1,+1\}$. She wants to know if the test error is greater than the training error. Your co-worker Cedric says it will help to evaluate $h(x)=\text{sign}(w\cdot x)$ for every $(x,y)\in D_{TR}$ and every $(x,y)\in D_{TE}$.
\subsubsection*{a)}
Does Cedric's suggestion help to determine whether the test error is higher than the training error? Why or why not? In your answer, use the formula for error rate.\\

\textbf{Yes. We can compare training error $\sum_{(x,y)\in D_{TR}} \ell_{0/1}(h(x),y)$ to test error $\sum_{(x,y)\in D_{TR}} \ell_{0/1}(h(x),y)$.}

\subsubsection*{b)}
Can you determine whether the test error is greater than the training error, without looking at all of the points $D_{TR}$ and $D_{TE}$? If so, how? If not, why?\\

\textbf{We do not need to evaluate training error. We know that this "fully trained" perceptron has zero training error because perceptrons finish training only once training error is zero.}

\subsection*{Problem 2}
% Learning goals: algorithm familiarity and algebra.
You are on a desert island and must run the perceptron algorithm \textit{without computational aid}. Consider the following two-point 2D dataset:
\begin{enumerate}
\item Positive class: $(10, -2)$
\item Negative class: $(12, 2)$
\end{enumerate}
Starting with $w_0=(0,0)$, how many updates to $w$ are needed until convergence? Write down each $w_i$ in the sequence.\\

\textbf{Nine updates. Depending on which example is chosen first, two sequences are possible:}

$[(0,0), (10,-2), (-2,-4), (8,-6), (-4,-8), (6,-10), (-6,-12), (4,-14), (-8,-16), (2,-18)]$

$[(0,0), (-12,-2), (-2,-4), (8,-6), (-4,-8), (6,-10), (-6,-12), (4,-14), (-8,-16), (2,-18)]$

\subsection*{Problem 3}
% Learning goals: "margin" (as used in convergence proof) vs "minimum distance", upper bound on number of updates, linear separability requirement.
Your friend Chiku is quizzing you on how to use the proof from class to upper bound the convergence time of the perceptron algorithm. She gives you the following 3-point 2D dataset:
\begin{enumerate}
\item Positive class: $(-1, 1)$
\item Negative class: $(1,0)$, $(0,-1)$
\end{enumerate}
\subsubsection*{a)}
What is the minimum distance between these points and the hyperplane defined by $w=(-1,1)$? Why is this \textit{not} the $\gamma$ defined in the proof?\\

\textbf{All points happen to be equidistant from the hyperplane, with a distance of $\sqrt[]{2}$. This is not $\gamma$ because the points have not been rescaled to lie within the unit circle.}

\subsubsection*{b)}
Find a valid value of $\gamma$. Using the theorem from class, what is an upper bound on the number of updates the perceptron will make?\\

\textbf{Rescale by dividing each point by the largest norm $||(-1,1)||=\sqrt[]{2}$. Now the minimum distance from the hyperplane is $\gamma=1/2$, and the upper bound is $1/\gamma^2=4$.}

\subsubsection*{c)}
Chiku adds the point $(1,-1)$ to the positive class. What is the upper bound on the number of updates the perceptron will make?\\

\textbf{The limit does not exist. The dataset is no longer linearly separable, and thus the perceptron will not converge.}

\subsection*{Problem 4}
% Learning goals: weight vector is linear combination of training dataset.
Your friend Cuneyt comes to you, desperate for your Perceptron expertise. His dataset is massive (with 10+ trillion training examples), and after hours of training his perceptron (until convergence), his code did not save the final weight vector.

Thankfully, at every training iteration, the code saves which example is used for the update step. Surprisingly, only five of the 10+ trillion training examples were ever misclassified. They are listed below, along with the number of times they were used in an update step.

\begin{center}
\begin{tabular}{ c c }
 Training Example & Number of Times Used in an Update Step \\ 
 \hline
 (0, 0, 0, 0, 4),	+1 & 2  \\
 (0, 0, 6, 5, 0),	+1 & 1  \\
 (3, 0, 0, 0, 0),	-1 & 1  \\
 (0, 9, 3, 6, 0),	-1 & 1  \\ 
 (0, 1, 0, 2, 5),	-1 & 1     
\end{tabular}
\end{center}

What is the final weight vector for Cuneyt's perceptron?\\

\textbf{(-3, -10, 3, -3, 3)}














\end{document}