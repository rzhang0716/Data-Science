{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting \n",
        "Gradient Boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems."
      ],
      "metadata": {
        "id": "nXxl7h90gGoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting with Scikit-Learn"
      ],
      "metadata": {
        "id": "JN76SxDMo1e5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Gradient Boosting Machine for Classification"
      ],
      "metadata": {
        "id": "KxFunZVfpgSI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlQetGotc0Hr",
        "outputId": "3bcc16d3-4880-4f8b-ecfe-ff18751c5ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.914 (0.027)\n",
            "Prediction: 1\n"
          ]
        }
      ],
      "source": [
        "# gradient boosting for classification in scikit-learn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = GradientBoostingClassifier()\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %d' % yhat[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Gradient Boosting Machine for Regression"
      ],
      "metadata": {
        "id": "4s5SQWOTpjyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient boosting for regression in scikit-learn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = GradientBoostingRegressor()\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %.3f' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DatfG16kpc83",
        "outputId": "4e1a1250-9948-466f-9df8-193c69d6ba2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: -11.853 (1.128)\n",
            "Prediction: -80.661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Histogram-Based Gradient Boosting for Classification\n",
        "\n",
        "This is inspired by LightGBM. The primiary benefit of the histogram-based approach to gradient boosting is speed. Much faster to fit on training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "PtHQYvbQp0w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram-based gradient boosting for classification in scikit-learn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = HistGradientBoostingClassifier()\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = HistGradientBoostingClassifier()\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %d' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTMN4ahepnmz",
        "outputId": "74de8625-0d6d-4c42-b877-9205727a0246"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:17: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  \"Since version 1.0, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.934 (0.027)\n",
            "Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Histogram-Based Gradient Boosting for Regression"
      ],
      "metadata": {
        "id": "CIP__QzjqT6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram-based gradient boosting for regression in scikit-learn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = HistGradientBoostingRegressor()\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = HistGradientBoostingRegressor()\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %.3f' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QqwUJIDqKaY",
        "outputId": "572900fe-b78d-4a01-9af3-0f09bd23d03d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: -12.723 (1.540)\n",
            "Prediction: -77.837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting with XGboost\n",
        "\n",
        "XGBoost is short for *\"Extreme Gradient Boosting\"*, is a library that provides an efficient implementation of the gradient boosting algorithm."
      ],
      "metadata": {
        "id": "sceQAgZiqgSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. XGBoost for classification"
      ],
      "metadata": {
        "id": "d_OqdaCAqxKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xgboost for classification\n",
        "from numpy import asarray\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = XGBClassifier()\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = XGBClassifier()\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]\n",
        "row = asarray(row).reshape((1, len(row)))\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %d' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckTE7VgdqcTt",
        "outputId": "b1c976b1-e21e-4153-9dc6-880730019371"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.919 (0.026)\n",
            "Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost for Regression"
      ],
      "metadata": {
        "id": "BZzpAG9cq4dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xgboost for regression\n",
        "from numpy import asarray\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]\n",
        "row = asarray(row).reshape((1, len(row)))\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %.3f' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unnAAjPPq0tF",
        "outputId": "d157c485-77fc-4142-964d-c0404f3e9729"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: -12.083 (1.259)\n",
            "Prediction: -77.347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting with LightGBM\n",
        "\n",
        "LightGBM is short for Light Gradient Boosted Machine, which makes the algorithm faster."
      ],
      "metadata": {
        "id": "hM7hYFHSrH1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM for Classification"
      ],
      "metadata": {
        "id": "XQqiwuX5r8_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lightgbm for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = LGBMClassifier()\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = LGBMClassifier()\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %d' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDkKN0Urq-M6",
        "outputId": "c4eed424-666f-489b-9595-aa8296b51d13"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.934 (0.021)\n",
            "Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM for Regression"
      ],
      "metadata": {
        "id": "oASnFyIysDqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lightgbm for regression\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = LGBMRegressor()\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = LGBMRegressor()\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %.3f' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCKPZFYtr7cx",
        "outputId": "fc3fc49a-7262-42b7-a876-ca0eba96b79e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: -12.739 (1.408)\n",
            "Prediction: -82.040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CatBoost - Category Gradient Boosting\n",
        "\n",
        "The primary benefit of the CatBoost is support for categorical input variables. "
      ],
      "metadata": {
        "id": "fLf5jIy5sH5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. CatBoost for Classification"
      ],
      "metadata": {
        "id": "5XjD80f6sUeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xF2miCXsZm5",
        "outputId": "a70a9765-8cad-481c-f878-e2fad7504e38"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.1-cp37-none-manylinux1_x86_64.whl (76.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# catboost for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = CatBoostClassifier(verbose=0, n_estimators=100)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = CatBoostClassifier(verbose=0, n_estimators=100)\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %d' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_nnpiggsGtj",
        "outputId": "ee0cc4f5-af1e-45ac-f4d5-fe102aafc90f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.925 (0.025)\n",
            "Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. CatBoost for Regression"
      ],
      "metadata": {
        "id": "hnuMLEPwsfBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# catboost for regression\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
        "# evaluate the model\n",
        "model = CatBoostRegressor(verbose=0, n_estimators=100)\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "# fit the model on the whole dataset\n",
        "model = CatBoostRegressor(verbose=0, n_estimators=100)\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %.3f' % yhat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROHW6Dn7sXAa",
        "outputId": "1994a0bd-bdef-4b90-fb83-474010051fac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: -9.701 (1.234)\n",
            "Prediction: -81.905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-D3U-ia4sjBJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}