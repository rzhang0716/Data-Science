{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark-DataExploration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Univariate Analysis"
      ],
      "metadata": {
        "id": "0wN7FntynpmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Numerical Variables"
      ],
      "metadata": {
        "id": "gj6rTuV2n2T-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy9ZKpJonWwj"
      },
      "outputs": [],
      "source": [
        "# Samae as pandas, PySpark also use .describe to show information\n",
        "num_cols = ['Account Balance','No of dependents']\n",
        "df.select(num_cols).describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As PySpark does not include the quartiles, crate a function to get the same results in pandas\n",
        "def describe_pd(df_in, columns, deciles=False):\n",
        "  '''\n",
        "  Function to union the basic stats results and deciles\n",
        "  :param df_in: the input dataframe\n",
        "  :param columns: the cloumn name list of the numerical variable\n",
        "  :param deciles: the deciles output\n",
        "  :return : the numerical describe info. of the input dataframe\n",
        "  '''\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "\n",
        "  \n",
        "  if deciles:\n",
        "    percentiles = np.array(range(0, 110, 10))\n",
        "  else:\n",
        "    percentiles = [25, 50, 75]\n",
        "\n",
        "  percs = np.transpose([np.percentile(df_in.select(x).collect(),percentiles) for x in columns])\n",
        "  percs = pd.DataFrame(percs, columns=columns)\n",
        "  percs['summary'] = [str(p) + '%' for p in percentiles]\n",
        "  spark_describe = df_in.describe().toPandas()\n",
        "  new_df = pd.concat([spark_describe, percs],ignore_index=True)\n",
        "  new_df = new_df.round(2)\n",
        "  return new_df[['summary'] + columns]"
      ],
      "metadata": {
        "id": "U49rgoP1oMTb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Categorical Variables"
      ],
      "metadata": {
        "id": "mfkM3XJZoKuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Frequency Table"
      ],
      "metadata": {
        "id": "LaZ6BY1RqblD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import rank,sum,col\n",
        "from pyspark.sql import Window\n",
        "window = Window.rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
        "# withColumn('Percent %',F.format_string(\"%5.0f%%\\n\",col('Credit_num')*100/col('total'))).\n",
        "tab = df.select(['age_class','Credit Amount']).\\\n",
        "groupBy('age_class').\\\n",
        "agg(F.count('Credit Amount').alias('Credit_num'),\n",
        "F.mean('Credit Amount').alias('Credit_avg'),\n",
        "F.min('Credit Amount').alias('Credit_min'),\n",
        "F.max('Credit Amount').alias('Credit_max')).\\\n",
        "withColumn('total',sum(col('Credit_num')).over(window)).\\\n",
        "withColumn('Percent',col('Credit_num')*100/col('total')).\\\n",
        "drop(col('total'))"
      ],
      "metadata": {
        "id": "v-LQpwhBqa5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "tfnUBeZSrVlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariate Analysis"
      ],
      "metadata": {
        "id": "uuE1g1alrWVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Numerical vs. Numerical"
      ],
      "metadata": {
        "id": "_qm0qQeirZbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Correlation matrix"
      ],
      "metadata": {
        "id": "jf7TRWO-rctY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.stat import Statistics\n",
        "import pandas as pd\n",
        "\n",
        "corr_data = df.select(num_cols)\n",
        "\n",
        "col_names = corr_data.columns\n",
        "features = corr_data.rdd.map(lambda row: row[0:])\n",
        "corr_mat=Statistics.corr(features, method=\"pearson\")\n",
        "\n",
        "corr_df = pd.DataFrame(corr_mat)\n",
        "corr_df.index, corr_df.columns = col_names, col_names\n",
        "\n",
        "print(corr_df.to_string())"
      ],
      "metadata": {
        "id": "BwHFo6iErV_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Categorical vs. Categorical"
      ],
      "metadata": {
        "id": "YaVdTyI-sLt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Pearson's Chi-squared test"
      ],
      "metadata": {
        "id": "mIet0JUhsQnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
        "(0.0, Vectors.dense(1.5, 20.0)),\n",
        "(1.0, Vectors.dense(1.5, 30.0)),\n",
        "(0.0, Vectors.dense(3.5, 30.0)),\n",
        "(0.0, Vectors.dense(3.5, 40.0)),\n",
        "(1.0, Vectors.dense(3.5, 40.0))]\n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))"
      ],
      "metadata": {
        "id": "D-NAqvKQsPFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross table"
      ],
      "metadata": {
        "id": "E2VFKhJ1tSGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.stat.crosstab(\"age_class\", \"Occupation\").show()"
      ],
      "metadata": {
        "id": "NPm9UQpztcw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical vs. Categorical"
      ],
      "metadata": {
        "id": "uSDKtDoGtg4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OytiKlH5tkiu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}