\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{mathabx}
\usepackage{tikz}

\setlength\parindent{0pt}

\title{CS 4780/5780 Homework 2}
\author{Due: Wednesday 02/21/18 11:55pm on Gradescope}
\date{}

\begin{document}
\maketitle

The following problems concern the perceptron. In class you learned when the perceptron may be applied, how to algorithmically train the perceptron, and when the perceptron converges.

\subsection*{Problem 1}
% Learning goals: perceptron prediction formula, error formula for perceptrons, training error is 0.
Suppose your boss Celeste gives you a fully trained (up to convergence) Perceptron ($w$), the training set used to train it ($D_{TR}$), and a separate test set ($D_{TE}$). Here $x\in\mathbb{R}^d$ and $y\in\{-1,+1\}$. She wants to know if the test error is greater than the training error. Your co-worker Cedric says it will help to evaluate $h(x)=\text{sign}(w\cdot x)$ for every $(x,y)\in D_{TR}$ and every $(x,y)\in D_{TE}$.
\subsubsection*{a)}
Does Cedric's suggestion help to determine whether the test error is higher than the training error? Why or why not? In your answer, use the formula for error rate.
\subsubsection*{b)}
Consider the Perceptron algorithm mentioned in class, why is there no need to compute the training error?

\subsection*{Problem 2}
% Learning goals: algorithm familiarity and algebra.
You are on a desert island and must run the perceptron algorithm \textit{without computational aid}. Consider the following two-point 2D dataset:
\begin{enumerate}
\item Positive class: $(10, -2)$
\item Negative class: $(12, 2)$
\end{enumerate}
Starting with $w_0=(0,0)$, how many updates to $w$ are needed until convergence? Write down each $w_i$ in the sequence.

\subsection*{Problem 3}
% Learning goals: "margin" (as used in convergence proof) vs "minimum distance", upper bound on number of updates, linear separability requirement.
Your friend Chiku is quizzing you on how to use the proof from class to upper bound the convergence time of the perceptron algorithm. She gives you the following 3-point 2D dataset:
\begin{enumerate}
\item Positive class: $(-1, 1)$
\item Negative class: $(1,0)$, $(0,-1)$
\end{enumerate}
\subsubsection*{a)}
What is the minimum distance between these points and the hyperplane defined by $w=(-1,1)$? Why is this \textit{not} the $\gamma$ defined in the proof?
\subsubsection*{b)}
Find a valid value of $\gamma$. Using the theorem from class, what is an upper bound on the number of updates the perceptron will make?
\subsubsection*{c)}
Chiku adds the point $(1,-1)$ to the positive class. What is the upper bound on the number of updates the perceptron will make?

\subsection*{Problem 4}
% Learning goals: weight vector is linear combination of training dataset.
Your friend Cuneyt comes to you, desperate for your Perceptron expertise. His dataset is massive (with 10+ trillion training examples), and after hours of training his perceptron (until convergence), his code did not save the final weight vector.

Thankfully, at every training iteration, the code saves which example is used for the update step. Surprisingly, only five of the 10+ trillion training examples were ever misclassified. They are listed below, along with the number of times they were used in an update step.

\begin{center}
\begin{tabular}{ c c }
 Training Example & Number of Times Used in an Update Step \\ 
 \hline
 (0, 0, 0, 0, 4),	+1 & 2  \\
 (0, 0, 6, 5, 0),	+1 & 1  \\
 (3, 0, 0, 0, 0),	-1 & 1  \\
 (0, 9, 3, 6, 0),	-1 & 1  \\ 
 (0, 1, 0, 2, 5),	-1 & 1     
\end{tabular}
\end{center}

What is the final weight vector for Cuneyt's perceptron?
















\end{document}