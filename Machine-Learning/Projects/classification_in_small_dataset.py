# -*- coding: utf-8 -*-
"""Megan_classification in small dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16XXrcmsjvuQdk3Oi9g9dlvYh-ph34XZb
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

import scipy.stats as stats

# %matplotlib inline

data = pd.read_excel('df.xlsx')

data.head()

# Convert treatment to factor
codes, uniques = pd.factorize(data['treatment'])
data['treatment'] = codes

data.describe()

data1 = data.drop(['treatment','patid'],axis=1)

# plot some data
fig,ax = plt.subplots(1,figsize=(12,4))
ax = sns.boxplot(data=data1)
ax.set_xticklabels(ax.get_xticklabels(),rotation=45)
plt.show()

data1.keys()

### z-score all variables except for quality

# find the columns we want to normalize (all except quality)
cols2zscore = data1.keys()


# z-score (written out for clarity)
for col in cols2zscore:
  meanval   = data1[col].mean()
  stdev     = data1[col].std()
  data1[col] = (data1[col]-meanval) / stdev

data1.describe()

# check the plot again
fig,ax = plt.subplots(1,figsize=(12,4))
ax = sns.boxplot(data=data1)
ax.set_xticklabels(ax.get_xticklabels(),rotation=45)
plt.show()

"""Wired value"""

data[data1['MIG_CXCL9']>3]

data[data1['Eotaxin2_CCL24']>3]

"""Outliers"""

data[data1['MIG_CXCL9']>1]

data[data1['Eotaxin2_CCL24']>1]

"""Seperate by group"""

df_control = data1.loc[data['treatment']==0]
df_control

df_trt = data1.loc[data['treatment']==1]

# check the plot with control group
fig,ax = plt.subplots(1,figsize=(12,4))
ax = sns.boxplot(data=df_control)
ax.set_xticklabels(ax.get_xticklabels(),rotation=45)
plt.show()

# check the plot with treatment group
fig,ax = plt.subplots(1,figsize=(17,4))
ax = sns.boxplot(data=df_trt)
ax.set_xticklabels(ax.get_xticklabels(),rotation=45)
plt.show()

df_trt[df_trt['MIG_CXCL9']>3]

df_trt[df_trt['Eotaxin2_CCL24']>3]

# Seperate still same as before, we can remove these two points for now to get a new dataset

data.drop(data[data1['Eotaxin2_CCL24']>3].index,inplace=True)

data.drop(data[data1['MIG_CXCL9']>3].index, inplace=True)

# Reset index
data.reset_index()

"""# Classification"""

import random 
random.seed(666)

"""## 1. Random Forest for Binary Classification"""

# No standardize for random forest
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from matplotlib import pyplot

X = data[['MIG_CXCL9','IP10_CXCL10','Eotaxin2_CCL24']]

y = data['treatment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test
clf=RandomForestClassifier()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""#### 1.2 Cross-validation"""

model = RandomForestClassifier()
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

"""#### 1.3 Hyperparameter tuning

#### (1) Explore number of samples
"""

# get a list of models to evaluate
from numpy import arange
def get_models():
	models = dict()
	# explore ratios from 10% to 100% in 10% increments
	for i in arange(0.1, 1.1, 0.1):
		key = '%.1f' % i
		# set max_samples=None to use 100%
		if i == 1.0:
			i = None
		models[key] = RandomForestClassifier(max_samples=i)
	return models
 
# evaluate a given model using cross-validation
def evaluate_model(model, X, y):
	# define the evaluation procedure
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
	# evaluate the model and collect the results
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores

# evaluate the models and store results
models = get_models()
results, names = list(), list()
for name, model in models.items():
	# evaluate the model
	scores = evaluate_model(model, X, y)
	# store the results
	results.append(scores)
	names.append(name)
	# summarize the performance along the way
	print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

"""#### (2) Explore number of trees"""

# get a list of models to evaluate
def get_models():
	models = dict()
	# define number of trees to consider
	n_trees = [10, 50, 100, 500, 1000]
	for n in n_trees:
		models[str(n)] = RandomForestClassifier(n_estimators=n)
	return models
 
# evaluate a given model using cross-validation
def evaluate_model(model, X, y):
	# define the evaluation procedure
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
	# evaluate the model and collect the results
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores

models = get_models()
# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
	# evaluate the model
	scores = evaluate_model(model, X, y)
	# store the results
	results.append(scores)
	names.append(name)
	# summarize the performance along the way
	print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

"""#### (3) Explore tree depth"""

# get a list of models to evaluate
def get_models():
	models = dict()
	# consider tree depths from 1 to 7 and None=full
	depths = [i for i in range(1,8)] + [None]
	for n in depths:
		models[str(n)] = RandomForestClassifier(max_depth=n)
	return models
 
# evaluate a given model using cross-validation
def evaluate_model(model, X, y):
	# define the evaluation procedure
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
	# evaluate the model and collect the results
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores
  
# get the models to evaluate
models = get_models()
# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
	# evaluate the model
	scores = evaluate_model(model, X, y)
	# store the results
	results.append(scores)
	names.append(name)
	# summarize the performance along the way
	print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

"""#### (4) Random Search"""

from sklearn.model_selection import RandomizedSearchCV
random_grid = {'bootstrap': [True, False],
 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [100, 200, 300, 400, 500]}

# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestClassifier()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=1, n_jobs = -1)
# Fit the random search model
rf_random.fit(X, y)
print(rf_random.best_params_)

model = RandomForestClassifier(n_estimators=400, min_samples_leaf=4, min_samples_split=2, max_features='sqrt',max_depth=10, bootstrap=True)
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

"""---

## 2. K-Nearest Neighbors
"""

# Normalization is required for KNN

from sklearn import preprocessing
X_normal = preprocessing.normalize(X)
X_train, X_test, y_train, y_test = train_test_split(X_normal, y, test_size=0.3) # 70% training and 30% test

from sklearn.model_selection import GridSearchCV
 from sklearn.neighbors import KNeighborsClassifier
parameters = {"n_neighbors": range(1, 10)}
gridsearch = GridSearchCV(KNeighborsClassifier(), parameters)
gridsearch.fit(X_train, y_train)

print(gridsearch.best_estimator_)

# Use the KNN classifier to fit data:
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
classifier = KNeighborsClassifier(n_neighbors=7)
classifier.fit(X_train, y_train) 

# Predict y data with classifier: 
y_predict = classifier.predict(X_test)

# Print results: 
print(confusion_matrix(y_test, y_predict))
print(classification_report(y_test, y_predict)) 
print(accuracy_score(y_test, y_predict))

model = KNeighborsClassifier(n_neighbors=7)
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X_normal, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

"""# 3. Naive Bayes"""

from sklearn import preprocessing
X_normal = preprocessing.normalize(X)
X_train, X_test, y_train, y_test = train_test_split(X_normal, y, test_size=0.3)

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

cm = confusion_matrix(y_test, y_pred)
ac = accuracy_score(y_test,y_pred)

print(ac)

model = GaussianNB()
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X_normal, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

"""# 4. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(penalty='l2')
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

"""# 5. Support Vector Machine"""

# Standardize before processing 
scaler = preprocessing.StandardScaler().fit(X)
X_scale = scaler.transform(X)

from sklearn.svm import SVC
from scipy.stats import reciprocal, uniform

# Train and test split
X_train, X_test, y_train, y_test = train_test_split(X_scale,y,test_size = 0.30, random_state = 1)

# defining parameter range
param_distributions = {"gamma": reciprocal(0.001, 0.1), "C": uniform(1, 1000),'kernel': ['linear','poly','rbf','sigmoid']}
 
grid = RandomizedSearchCV(SVC(), param_distributions, refit = True, verbose = 1)
 
# fitting the model for grid search
grid.fit(X_train, y_train)

# print best parameter after tuning
print(grid.best_params_)
 
# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)

grid_predictions = grid.predict(X_test)
 
# print classification report
print(classification_report(y_test, grid_predictions))
print(accuracy_score(y_test, grid_predictions))

model = SVC()
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

"""# 6. LightGBM"""

import numpy as np
from collections import Counter
import pandas as pd
import lightgbm as lgb


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import mean_squared_error,roc_auc_score,precision_score
pd.options.display.max_columns = 999

# Standardize before processing 
scaler = preprocessing.StandardScaler().fit(X)
X_scale = scaler.transform(X)
# Train and test split

X_train, X_test, y_train, y_test = train_test_split(X_scale,y,test_size = 0.30, random_state = 1)

from lightgbm import LGBMClassifier
model = LGBMClassifier()
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X_scale, y, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

"""#### Hyperparameter Tuning

#### (1) Explore Number of Trees
"""

# get a list of models to evaluate
def get_models():
	models = dict()
	trees = [1, 3, 5, 7, 8, 9]
	for n in trees:
		models[str(n)] = LGBMClassifier(n_estimators=n)
	return models
 
# evaluate a give model using cross-validation
def evaluate_model(model):
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores
 
# get the models to evaluate
models = get_models()
# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
	scores = evaluate_model(model)
	results.append(scores)
	names.append(name)
	print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

"""#### (2) Explore Depth of Trees"""

# get a list of models to evaluate
def get_models():
	models = dict()
	for i in range(1,11):
		models[str(i)] = LGBMClassifier(max_depth=i, num_leaves=2**i)
	return models
 
# evaluate a give model using cross-validation
def evaluate_model(model):
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores


# get the models to evaluate
models = get_models()
# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
	scores = evaluate_model(model)
	results.append(scores)
	names.append(name)
	print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

"""#### (3) Explore Learning Rate"""

# get a list of models to evaluate
def get_models():
	models = dict()
	rates = [0.0001, 0.001, 0.01, 0.1, 1.0]
	for r in rates:
		key = '%.4f' % r
		models[key] = LGBMClassifier(learning_rate=r)
	return models
 
# evaluate a give model using cross-validation
def evaluate_model(model):
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
	scores = cross_val_score(model, X_scale, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores
 

# get the models to evaluate
models = get_models()
# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
	scores = evaluate_model(model)
	results.append(scores)
	names.append(name)
	print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

"""#### (4) Explore Boosting Type"""

# get a list of models to evaluate
def get_models():
	models = dict()
	types = ['gbdt', 'dart', 'goss']
	for t in types:
		models[t] = LGBMClassifier(boosting_type=t)
	return models
 
# evaluate a give model using cross-validation
def evaluate_model(model):
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores


# get the models to evaluate
models = get_models()
# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
	scores = evaluate_model(model)
	results.append(scores)
	names.append(name)
	print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

